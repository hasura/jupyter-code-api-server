{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single vectorization and event trigger\n",
    "\n",
    "You can fetch raw files from S3 by using python connector to S3 using boto3 or using Hasura's S3 data connector. \n",
    "Advantage of using Hasura S3 connector is your data queries are secure with authentication and RBAC support out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive character splitter: divides the input into smaller chunks in a hierarchical manner.\n",
    "# If the output of the first iteration didn't produce chunks of desired size, \n",
    "# it recursively chunks with the different separator or criterion until the desired size is achieved.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunking(text, chunk_size=1000, chunk_overlap=500, separators=[\".\"]):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap, \n",
    "                                               separators=separators)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility - GraphQL query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the chunks to the database\n",
    "# Utility method to execute GQL queries\n",
    "import os\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def execute_gql(query, variable_values={}):\n",
    "    transport=RequestsHTTPTransport(\n",
    "        url=os.environ.get('HASURA_GRAPHQL_URL'),\n",
    "        headers={'x-hasura-admin-secret': os.environ.get('HASURA_GRAPHQL_ADMIN_SECRET')},\n",
    "    )\n",
    "    client = Client(\n",
    "        transport=transport,\n",
    "    )\n",
    "    query = gql(query)\n",
    "    return client.execute(query, variable_values=variable_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AWS s3 utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch file from S3 using Boto3\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to read pdf file and return text \n",
    "def read_pdf_file(f):\n",
    "    pdf = PdfReader(f)\n",
    "    resume_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        resume_text += page.extract_text()\n",
    "        resume_text += \"\\n\\n  \"\n",
    "    return resume_text\n",
    "\n",
    "# Function to fetch list of files from S3 using bucket name and prefix\n",
    "def read_list_of_files_from_bucket(bucket_name, prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    files = []\n",
    "    for obj in bucket.objects.filter(Prefix=prefix):\n",
    "        files.append(obj.key)\n",
    "    return files\n",
    "\n",
    "# Fetch requested file given bucket name, prefix and filename\n",
    "def read_file_from_bucket(bucket_name, prefix, filename):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    obj = bucket.Object(prefix + \"/\" + filename)\n",
    "    fs = obj.get()['Body'].read()\n",
    "    resume_text = read_pdf_file(fs)\n",
    "    return resume_text\n",
    "\n",
    "# Fetch requested file by uri (s3://<bucket_name>/<prefix>/<filename>)\n",
    "def read_file_from_uri(uri):\n",
    "    bucket_name = uri.split(\"/\")[2]\n",
    "    prefix = \"/\".join(uri.split(\"/\")[3:-1])\n",
    "    filename = uri.split(\"/\")[-1]\n",
    "    resume_text = read_file_from_bucket(bucket_name, prefix, filename)\n",
    "    return resume_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'insert_ResumeChunks': {'affected_rows': 2}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hasura GraphQL query to insert the chunks into the database\n",
    "query_insert_list_of_chunk = \"\"\"\n",
    "mutation InsertChunks($objects: [ResumeChunks_insert_input!]!) {\n",
    "  insert_ResumeChunks(objects: $objects) {\n",
    "    affected_rows\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "# -- Test\n",
    "# Comment this when not testing\n",
    "list_of_chunks = {\"objects\":[{\"application_id\": \"1\", \"chunk_id\": 1, \"content\": \"first chunk\"},\n",
    "\t\t\t\t\t\t{\"application_id\": \"1\", \"chunk_id\": 2, \"content\": \"second chunk\"}]}\n",
    "execute_gql(query_insert_list_of_chunk,\n",
    "            variable_values=list_of_chunks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hasura workflow and event trigger end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'insert_ResumeChunks': {'affected_rows': 10}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hasura.workflow import Step, Workflow\n",
    "\n",
    "application_id = 4\n",
    "test_workflow1 = Workflow(name=\"write_chunked_resume_to_db\", elements=[\n",
    "                Step(name=\"read_file\", function=read_pdf_file, required_args=[\"f\"], return_result=True),\n",
    "                Step(name=\"chunking\", function=chunking, return_result=True),\n",
    "                Step(name=\"structure_chunks\", function=lambda input_chunks: {\"variable_values\":{\"objects\":\n",
    "                                                                                [ {\n",
    "                                                                                    \"application_id\": str(application_id),\n",
    "                                                                                    \"chunk_id\": idx+1,\n",
    "                                                                                    \"content\": c\n",
    "                                                                                    } for idx, c in enumerate(input_chunks) ]}}, \n",
    "                                                                            return_result=True),\n",
    "                Step(name=\"write_to_db\", function=execute_gql, required_args=[\"query\"])\n",
    "])  \n",
    "\n",
    "test_workflow1.execute(f=\"sample.pdf\", query=query_insert_list_of_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully deployed workflow at http://'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deploy workflow for event trigger\n",
    "test_workflow1.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bulk vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test integration with S3\n",
    "# read_file_from_bucket(bucket_name=\"\", prefix=\"\", filename=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "test_workflow2 = Workflow(name=\"write_chunked_resume_to_db\", elements=[\n",
    "                Step(name=\"read_file\", function=read_file_from_bucket, required_args=[\"bucket_name\",\n",
    "                                                                                      \"prefix\",\n",
    "                                                                                      \"filename\"], return_result=True),\n",
    "                Step(name=\"chunking\", function=chunking, return_result=True),\n",
    "                Step(name=\"structure_chunks\", function=lambda input_chunks: {\"variable_values\":{\"objects\":\n",
    "                                                                                [ {\n",
    "                                                                                    \"application_id\": str(application_id),\n",
    "                                                                                    \"chunk_id\": idx+1,\n",
    "                                                                                    \"content\": c\n",
    "                                                                                    } for idx, c in enumerate(input_chunks) ]}}, \n",
    "                                                                            return_result=True),\n",
    "                Step(name=\"write_to_db\", function=execute_gql, required_args=[\"query\"])\n",
    "])  \n",
    "\n",
    "test_workflow1.execute(bucket_name=\"\", prefix=\"\", filename=\"\", query=query_insert_list_of_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing of files but not concurrent\n",
    "\n",
    "bucket_name = \"\"\n",
    "prefix = \"\"\n",
    "\n",
    "for filename in read_list_of_files_from_bucket(bucket_name, prefix):\n",
    "    test_workflow2.execute(bucket_name=bucket_name, \n",
    "                      prefix=prefix, \n",
    "                      filename=filename, \n",
    "                      query=query_insert_list_of_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing of files with concurrency\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for file in read_list_of_files_from_bucket(bucket_name, prefix):\n",
    "        executor.submit(test_workflow2.execute, \n",
    "                        bucket_name=bucket_name, \n",
    "                        prefix=prefix, \n",
    "                        filename=file, \n",
    "                        query=query_insert_list_of_chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
