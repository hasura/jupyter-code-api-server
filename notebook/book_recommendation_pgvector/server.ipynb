{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hello\": \"world\"}\n"
     ]
    }
   ],
   "source": [
    "# GET /hello_world\n",
    "\n",
    "import json\n",
    "print(json.dumps({\n",
    "  'hello': 'world'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template for event trigger to ETL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /handle_event\n",
    "\n",
    "GRAPHQL_ENDPOINT = \"\"\n",
    "ADMIN_SECRET = \"\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import openai\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def get_embedding(text_to_embed):\n",
    "\t# Embed a line of text\n",
    "\tresponse = openai.Embedding.create(\n",
    "    \tmodel= \"text-embedding-ada-002\",\n",
    "    \tinput=[text_to_embed]\n",
    "\t)\n",
    "\t# Extract the AI output embedding as a list of floats\n",
    "\tembedding = response[\"data\"][0][\"embedding\"]\n",
    "\treturn embedding\n",
    "\n",
    "def handle_insert(row, client):\n",
    "    book_id = row['id']\n",
    "    description = row['description']\n",
    "    embedding = get_embedding(description)\n",
    "\n",
    "    gql_query = gql(\"\"\"\n",
    "            mutation insertEmbeddings($id: bigint!, $embedding: String!) {\n",
    "                insert_books_embedding(args: { book_id: $id, embedding: $embedding }) {\n",
    "                    id\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "    print(client.execute(gql_query, variable_values={\n",
    "        'id': str(book_id), 'embedding': str(embedding)}))\n",
    "\n",
    "\n",
    "def handle_event(request):\n",
    "    print(\"handle_event function got called\")\n",
    "    gql_headers = {'x-hasura-admin-secret': ADMIN_SECRET}\n",
    "    # Create a GraphQL client with the request transport\n",
    "    transport = RequestsHTTPTransport(\n",
    "        url=GRAPHQL_ENDPOINT, headers=gql_headers)\n",
    "    client = Client(transport=transport)\n",
    "\n",
    "    event = request['body']['event']\n",
    "    op = event['op']\n",
    "    if op == 'INSERT':\n",
    "        row = event['data']['new']\n",
    "        handle_insert(row, client)\n",
    "    else:\n",
    "        print(str(event))\n",
    "    return request\n",
    "\n",
    "print(json.dumps(handle_event(json.loads(REQUEST))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /handle_query\n",
    "\n",
    "GRAPHQL_ENDPOINT = \"\"\n",
    "ADMIN_SECRET = \"\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import openai\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def get_embedding(text_to_embed):\n",
    "\t# Embed a line of text\n",
    "\tresponse = openai.Embedding.create(\n",
    "    \tmodel= \"text-embedding-ada-002\",\n",
    "    \tinput=[text_to_embed]\n",
    "\t)\n",
    "\t# Extract the AI output embedding as a list of floats\n",
    "\tembedding = response[\"data\"][0][\"embedding\"]\n",
    "\treturn embedding\n",
    "\n",
    "def handle_query(request):\n",
    "    user_query = request['body']['input']['user_query']\n",
    "\n",
    "    gql_headers = dict()\n",
    "    gql_headers['x-hasura-admin-secret'] = ADMIN_SECRET\n",
    "\n",
    "    # Create a GraphQL client with the request transport\n",
    "    transport = RequestsHTTPTransport(\n",
    "        url=GRAPHQL_ENDPOINT, headers=gql_headers)\n",
    "    client = Client(transport=transport)\n",
    "\n",
    "    query_embedding = get_embedding(user_query)\n",
    "    # Send the GraphQL request\n",
    "    gql_query = gql(\"\"\"\n",
    "            query getSimilarBooks($query_embedding: String!) {\n",
    "                find_books_by_desc_similarity(args: { book_query_embedding: $query_embedding }) {\n",
    "                    id\n",
    "                    name\n",
    "                    genre\n",
    "                    description\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "    result = client.execute(gql_query, variable_values={\n",
    "                            'query_embedding': str(query_embedding)})\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are helpful book recommendation bot. You are given a user query and a list of books.\n",
    "    Use the following context to recommend the best books to the user and explain why.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \\n\n",
    "    \"\"\"\n",
    "    prompt += user_query\n",
    "\n",
    "    books = result[\"find_books_by_desc_similarity\"]\n",
    "\n",
    "    for b in books:\n",
    "        prompt += \"Book Name:\"\n",
    "        prompt += b[\"name\"]\n",
    "        prompt += \"Book Description: \"\n",
    "        prompt += b[\"description\"]\n",
    "        prompt += \"Book Genre: \"\n",
    "        prompt += b[\"genre\"]\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    prompt += \"\\nQuestion: {question}\"\n",
    "\n",
    "    llm = OpenAI(model=\"text-davinci-003\",\n",
    "                openai_api_key=OPENAI_API_KEY)\n",
    "    chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt))\n",
    "\n",
    "    return chain.run({\"question\":user_query})\n",
    "\n",
    "print(json.dumps(handle_query(json.loads(REQUEST))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
