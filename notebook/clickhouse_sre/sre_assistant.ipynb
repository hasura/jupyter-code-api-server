{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /sre_query\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import openai\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "# ----- Keys ------ #\n",
    "OPENAI_API_KEY = \"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\"\n",
    "HASURA_GRAPHQL_URL=\"https://proud-reptile-98.hasura.app/v1/graphql\"\n",
    "PAT_TOKEN=\"a1dHttDLrrUp7CjI71Hmn5wkFSST9R7RwQ8arBEQIfOrEOemSli3jlRndw90EjhE\"\n",
    "HASURA_GRAPHQL_ADMIN_SECRET=\"oWal4k1A3UYjevec5r2kImz6PbIUEzArwPOqaGaoQ4E8ZVCPau3av9nAI2w6Sbor\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# ----- Additional headers to pass in Hasura request ------ #\n",
    "additional_headers = {}\n",
    "# additional_headers = {'x-hasura-project-id': 'Project_2',\n",
    "#                       'x-hasura-role': 'sre'}\n",
    "\n",
    "\n",
    "# ----- Hasura utility ------ #\n",
    "class HasuraClient:\n",
    "    def __init__(self,hasura_graphql_url, hasura_graphql_admin_secret, \n",
    "                 hasura_graphql_pat_token, additional_headers={}):\n",
    "        headers = {'x-hasura-admin-secret': hasura_graphql_admin_secret,\n",
    "                     'Authorization': 'pat {token}'.format(token=hasura_graphql_pat_token)}\n",
    "        headers.update(additional_headers)\n",
    "\n",
    "        transport=RequestsHTTPTransport(\n",
    "            url=hasura_graphql_url,\n",
    "            headers=headers,\n",
    "        )\n",
    "        self.client = Client(\n",
    "            transport=transport\n",
    "        )\n",
    "\n",
    "    def execute_gql(self, query, variable_values={}):\n",
    "        query = gql(query)\n",
    "        return self.client.execute(query, variable_values=variable_values)\n",
    "    \n",
    "hcl = HasuraClient(HASURA_GRAPHQL_URL, HASURA_GRAPHQL_ADMIN_SECRET, PAT_TOKEN, additional_headers)\n",
    "\n",
    "# ----- Function schema for entity extraction from question ------ #\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"fetch_incident_details\", \n",
    "        \"description\": \"Identifies if the user is asking for analysis of an incident ticket and extracts the incident id\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"analysis_requested\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Boolean flag to indicate whether the user has requested for analysis.\"\n",
    "                },\n",
    "                \"incident_id\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Incident id from the text on which the user has requested for analysis.\"\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "] \n",
    "\n",
    "# ----- Queries for context ------ #\n",
    "request_query = \"\"\"\n",
    "query MyQuery($IncidentId: Int64!) {\n",
    "  incident(where: {incident_id: {_eq: $IncidentId}}) {\n",
    "    incident_id\n",
    "    request_id\n",
    "    timestamp\n",
    "    incident_request_relationship {\n",
    "      cpu_usage\n",
    "      execution_time\n",
    "      memory_usage\n",
    "      number_of_active_requests\n",
    "      request_id\n",
    "      server_id\n",
    "      project_id\n",
    "      timestamp\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_stats_query = \"\"\"\n",
    "query MyQuery($StartTime: DateTime64!, $EndTime: DateTime64!, $ServerId: String!) {\n",
    "  aggregated_server_metrics(where: {timestamp: {_gt: $StartTime, _lt: $EndTime}, server_id: {_eq: $ServerId}}) {\n",
    "    avg_cpu_usage\n",
    "    avg_memory_usage\n",
    "    server_id\n",
    "    timestamp\n",
    "    total_requests\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def fetch_db_data(incident_id, duration=6):\n",
    "    request_data = hcl.execute_gql(request_query, {\"IncidentId\":incident_id})\n",
    "    if request_data['incident'] == []:\n",
    "        return None\n",
    "    \n",
    "    time_of_incident = request_data['incident'][0]['timestamp'][:-10]\n",
    "    time_of_incident = datetime.strptime(time_of_incident, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    n_hours_before_incident = time_of_incident - timedelta(hours=duration)\n",
    "    n_hours_after_incident = time_of_incident + timedelta(hours=duration)\n",
    "    server_id = request_data['incident'][0]['incident_request_relationship']['server_id']\n",
    "\n",
    "    server_stats = hcl.execute_gql(system_stats_query, {\"StartTime\": n_hours_before_incident.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                                      \"EndTime\": n_hours_after_incident.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                                      \"ServerId\": server_id})\n",
    "\n",
    "    return request_data, server_stats\n",
    "\n",
    "def generate_context_augmented_question(incident_details):\n",
    "    prompt = \"\"\"\n",
    "                You are provided with request and server stats data for the incident.\n",
    "                Can you run RCA and identify the root cause of the incident?\n",
    "\n",
    "                Request data: \n",
    "                {request_data}\n",
    "\n",
    "                Server stats data:\n",
    "                {server_stats}\n",
    "\n",
    "                Analyse the data step by step and list down your analysis in bullet.\n",
    "                Share next steps to resolve the incident.\n",
    "                        \n",
    "            \"\"\"\n",
    "    incident_id = str(incident_details[\"incident_id\"])\n",
    "    request_data, server_stats = fetch_db_data(incident_id)\n",
    "    prompt = prompt.format(request_data=request_data, server_stats=server_stats)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def call_llm(question, functions=functions):\n",
    "    global OPENAI_API_KEY\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question }],\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "    )\n",
    "    message = response[\"choices\"][0][\"message\"]\n",
    "    if message.get(\"function_call\"):\n",
    "        incident_details = json.loads(message[\"function_call\"]['arguments'])\n",
    "        new_question = generate_context_augmented_question(incident_details)\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=[{\"role\": \"user\", \"content\": new_question }],\n",
    "        )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def handle_sre_question(request):\n",
    "    question = request['body']['input']['question']\n",
    "\n",
    "    answer = call_llm(question)\n",
    "    return \"answer\"\n",
    "\n",
    "print(json.dumps(handle_sre_question(json.loads(REQUEST))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Analysis:\\n- The request data shows that there was one incident with incident_id 3 and request_id 6 on August 1, 2023, at 00:00:00.000000000. The incident was related to a request handled by Server_2 for Project_3.\\n- The CPU usage during the incident was 64.01%, the execution time was 22.24 seconds, the memory usage was 68.56%, and there were 77 active requests at that time.\\n\\n- The server stats data shows the aggregated metrics for Server_2 at different timestamps on August 1, 2023. The average CPU usage varied from 49.66% to 52.62%, and the average memory usage varied from 45.88% to 54.64%.\\n- The total number of requests handled by Server_2 ranged from 5575 to 6286 during the different timestamps.\\n\\nNext steps to resolve the incident:\\n1. Identify if the incident was caused by a server overload or performance issue. This can be done by comparing the CPU and memory usage during the incident with the average usage at that timestamp. If the incident's CPU and memory usage are significantly higher than the averages, it indicates a potential server performance issue.\\n2. Analyze the number of active requests during the incident. If the number of active requests during the incident is abnormally high compared to the average number of requests, it could be a contributing factor to the incident.\\n3. Evaluate the execution time of the request during the incident. If the execution time is significantly longer than the average execution time, it could indicate a performance bottleneck causing the incident.\\n4. Investigate any patterns or trends in the server stats data. Look for any specific timestamps or trends where the server metrics show a significant change or anomaly that could be related to the incident.\\n5. Consider other factors outside the provided data that could impact the incident, such as network connectivity issues or dependencies on external systems.\\n\\nBased on the above analysis, the next steps to resolve the incident would be:\\n1. Perform a detailed analysis of Server_2's performance metrics, including CPU and memory usage, for the specific incident timestamp. Compare it with the average values to identify any abnormalities.\\n2. Investigate the request execution time and compare it with the average execution time for similar requests to identify any performance bottlenecks.\\n3. Analyze the network connectivity and any external dependencies that could have contributed to the incident.\\n4. Ensure that Server_2 is properly optimized and configured to handle the expected workload, including any necessary upgrades or adjustments to accommodate the number of active requests.\\n5. Implement monitoring and alerting measures to detect similar incidents in the future and proactively address them.\\n\\nNote: The provided data is limited, and further analysis and investigation may be required to determine the precise root cause of the incident.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llm(\"analysis incident 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
